{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmed-boutar/adverserial-patch/blob/main/adversial_patch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, I will be generating an Adversarial Patch, in order to fool the RES34 model and have it classify different images to which the patch was added, with a wrong label.\n",
        "The code in this notebook was largely referenced from this colab notefbook, produced by Phillip Lippe. Source: https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb#scrollTo=y3loTwApmeNx\n",
        "\n",
        "The difference here is using a image mask in the training in order to produce stickers and not just a rectangular image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aYisT3bpRGWh"
      },
      "outputs": [],
      "source": [
        "# Please use this to connect your GitHub repository to your Google Colab notebook\n",
        "# Connects to any needed files from GitHub and Google Drive\n",
        "import os\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# Clone GitHub files to colab workspace\n",
        "repo_name = \"adversarial-patch\" # Change to your repo name\n",
        "git_path = 'https://github.com/ahmed-boutar/adversarial-patch.git' #Change to your path\n",
        "!git clone \"{git_path}\"\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "#%!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\" #Add if using requirements.txt\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "notebook_dir = 'src'\n",
        "path_to_notebook = os.path.join(repo_name,notebook_dir)\n",
        "%cd \"{path_to_notebook}\"\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing standard libraries and choosing the device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/b7/2dqsctv50sbgk3bg9gpz453c0000gn/T/ipykernel_4240/3204026054.py:13: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg', 'pdf') # For export\n",
            "Global seed set to 42\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device mps\n"
          ]
        }
      ],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np \n",
        "import scipy.linalg\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "# Pillow (to be used later for saving the patches as png files)\n",
        "from PIL import Image\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Fetching the device that will be used throughout this notebook\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(\"Using device\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download the dataset TinyImageNet, which is a subset of the ImageNet dataset. The dataset has been preprocessed and won't require resizing the different images. \n",
        "Download the pretrained res34 model (trained on TinyImageNet dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "import zipfile\n",
        "# Github URL where the dataset is stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/\"\n",
        "# Files to download\n",
        "pretrained_files = [(DATASET_PATH, \"TinyImageNet.zip\")]\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for dir_name, file_name in pretrained_files:\n",
        "    file_path = os.path.join(dir_name, file_name)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
        "        if file_name.endswith(\".zip\"):\n",
        "            print(\"Unzipping file...\")\n",
        "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(file_path.rsplit(\"/\",1)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "use common CNN architectures trained on the ImageNet dataset. Such models are luckily provided by PyTorch's torchvision package, and hence we just need to load the model of our preference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CNN architecture pretrained on ImageNet\n",
        "os.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\n",
        "pretrained_model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "# No gradients needed for the network\n",
        "pretrained_model.eval()\n",
        "for p in pretrained_model.parameters():\n",
        "    p.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the data (from TinyImageNet, a subset of ImageNet under the same license) and creating a data loader. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mean and Std from ImageNet\n",
        "NORM_MEAN = np.array([0.485, 0.456, 0.406])\n",
        "NORM_STD = np.array([0.229, 0.224, 0.225])\n",
        "# No resizing and center crop necessary as images are already preprocessed.\n",
        "plain_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=NORM_MEAN,\n",
        "                         std=NORM_STD)\n",
        "])\n",
        "\n",
        "# Load dataset and create data loader\n",
        "imagenet_path = os.path.join(DATASET_PATH, \"TinyImageNet/\")\n",
        "assert os.path.isdir(imagenet_path), f\"Could not find the ImageNet dataset at expected path \\\"{imagenet_path}\\\". \" + \\\n",
        "                                     f\"Please make sure to have downloaded the ImageNet dataset here, or change the {DATASET_PATH=} variable.\"\n",
        "dataset = torchvision.datasets.ImageFolder(root=imagenet_path, transform=plain_transforms)\n",
        "data_loader = data.DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True, num_workers=8)\n",
        "\n",
        "# Load label names to interpret the label numbers 0 to 999\n",
        "with open(os.path.join(imagenet_path, \"label_list.json\"), \"r\") as f:\n",
        "    label_names = json.load(f)\n",
        "    \n",
        "def get_label_index(lab_str):\n",
        "    assert lab_str in label_names, f\"Label \\\"{lab_str}\\\" not found. Check the spelling of the class.\"\n",
        "    return label_names.index(lab_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"The function below plots an image along with a bar diagram of its predictions. We also prepare it to show adversarial examples for later applications\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_prediction(img, label, pred, K=5, adv_img=None, noise=None):\n",
        "    \n",
        "    if isinstance(img, torch.Tensor):\n",
        "        # Tensor image to numpy\n",
        "        img = img.cpu().permute(1, 2, 0).numpy()\n",
        "        img = (img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
        "        img = np.clip(img, a_min=0.0, a_max=1.0)\n",
        "        label = label.item()\n",
        "    \n",
        "    # Plot on the left the image with the true label as title.\n",
        "    # On the right, have a horizontal bar plot with the top k predictions including probabilities\n",
        "    if noise is None or adv_img is None:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(10,2), gridspec_kw={'width_ratios': [1, 1]})\n",
        "    else:\n",
        "        fig, ax = plt.subplots(1, 5, figsize=(12,2), gridspec_kw={'width_ratios': [1, 1, 1, 1, 2]})\n",
        "    \n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title(label_names[label])\n",
        "    ax[0].axis('off')\n",
        "    \n",
        "    if adv_img is not None and noise is not None:\n",
        "        # Visualize adversarial images\n",
        "        adv_img = adv_img.cpu().permute(1, 2, 0).numpy()\n",
        "        adv_img = (adv_img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
        "        adv_img = np.clip(adv_img, a_min=0.0, a_max=1.0)\n",
        "        ax[1].imshow(adv_img)\n",
        "        ax[1].set_title('Adversarial')\n",
        "        ax[1].axis('off')\n",
        "        # Visualize noise\n",
        "        noise = noise.cpu().permute(1, 2, 0).numpy()\n",
        "        noise = noise * 0.5 + 0.5 # Scale between 0 to 1 \n",
        "        ax[2].imshow(noise)\n",
        "        ax[2].set_title('Noise')\n",
        "        ax[2].axis('off')\n",
        "        # buffer\n",
        "        ax[3].axis('off')\n",
        "    \n",
        "    if abs(pred.sum().item() - 1.0) > 1e-4:\n",
        "        pred = torch.softmax(pred, dim=-1)\n",
        "    topk_vals, topk_idx = pred.topk(K, dim=-1)\n",
        "    topk_vals, topk_idx = topk_vals.cpu().numpy(), topk_idx.cpu().numpy()\n",
        "    ax[-1].barh(np.arange(K), topk_vals*100.0, align='center', color=[\"C0\" if topk_idx[i]!=label else \"C2\" for i in range(K)])\n",
        "    ax[-1].set_yticks(np.arange(K))\n",
        "    ax[-1].set_yticklabels([label_names[c] for c in topk_idx])\n",
        "    ax[-1].invert_yaxis()\n",
        "    ax[-1].set_xlabel('Confidence')\n",
        "    ax[-1].set_title('Predictions')\n",
        "    \n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize a few images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exmp_batch, label_batch = next(iter(data_loader))\n",
        "with torch.no_grad():\n",
        "    preds = pretrained_model(exmp_batch.to(device))\n",
        "for i in range(1,17,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], preds[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "def place_patch(img, patch):\n",
        "    for i in range(img.shape[0]):\n",
        "        h_offset = np.random.randint(0,img.shape[2]-patch.shape[1]-1)\n",
        "        w_offset = np.random.randint(0,img.shape[3]-patch.shape[2]-1)\n",
        "        img[i,:,h_offset:h_offset+patch.shape[1],w_offset:w_offset+patch.shape[2]] = patch_forward(patch)\n",
        "        \n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The patch itself will be an nn.Parameter whose values are in the range between  −∞  and  ∞ . Images are, however, naturally limited in their range, and thus we write a small function that maps the parameter into the image value range of ImageNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "TENSOR_MEANS, TENSOR_STD = torch.FloatTensor(NORM_MEAN)[:,None,None], torch.FloatTensor(NORM_STD)[:,None,None]\n",
        "def patch_forward(patch):\n",
        "    global TENSOR_MEANS, TENSOR_STD\n",
        "    # Ensure TENSOR_MEANS and TENSOR_STD are on the correct device\n",
        "    TENSOR_MEANS = TENSOR_MEANS.to(device)\n",
        "    TENSOR_STD = TENSOR_STD.to(device)\n",
        "    # Ensure patch is on the correct device\n",
        "    patch = patch.to(device)\n",
        "    # Map patch values from [-infty,infty] to ImageNet min and max\n",
        "    patch = (torch.tanh(patch) + 1 - 2 * TENSOR_MEANS) / (2 * TENSOR_STD)\n",
        "    return patch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"Before looking at the actual training code, we can write a small evaluation function. We evaluate the success of a patch by how many times we were able to fool the network into predicting our target class. A simple function for this is implemented below\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_patch(model, patch, val_loader, target_class):\n",
        "    model.eval()\n",
        "    tp, tp_5, counter = 0., 0., 0.\n",
        "    with torch.no_grad():\n",
        "        for img, img_labels in tqdm(val_loader, desc=\"Validating...\", leave=False):\n",
        "            # For stability, place the patch at 4 random locations per image, and average the performance\n",
        "            for _ in range(4): \n",
        "                patch_img = place_patch(img, patch)\n",
        "                patch_img = patch_img.to(device)\n",
        "                img_labels = img_labels.to(device)\n",
        "                pred = model(patch_img)\n",
        "                # In the accuracy calculation, we need to exclude the images that are of our target class\n",
        "                # as we would not \"fool\" the model into predicting those\n",
        "                tp += torch.logical_and(pred.argmax(dim=-1) == target_class, img_labels != target_class).sum()\n",
        "                tp_5 += torch.logical_and((pred.topk(5, dim=-1)[1] == target_class).any(dim=-1), img_labels != target_class).sum()\n",
        "                counter += (img_labels != target_class).sum()\n",
        "    acc = tp/counter\n",
        "    top5 = tp_5/counter\n",
        "    return acc, top5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a function that will create a circular mask that will be placed on the image during training in order to produce a rounded patch (a sticker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_circular_mask(h, w, device, center=None, radius=None):\n",
        "    if center is None:  # use the middle of the image\n",
        "        center = (int(w/2), int(h/2))\n",
        "    if radius is None:  # use the smallest distance between the center and image walls\n",
        "        radius = min(center[0], center[1], w-center[0], h-center[1])\n",
        "\n",
        "    Y, X = torch.meshgrid(torch.arange(h, device=device), torch.arange(w, device=device))\n",
        "    dist_from_center = torch.sqrt((X - center[0])**2 + (Y-center[1])**2)\n",
        "\n",
        "    mask = dist_from_center <= radius\n",
        "    return mask.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [],
      "source": [
        "def patch_attack(model, target_class, patch_size=64, num_epochs=5):\n",
        "    # Leave a small set of images out to check generalization\n",
        "    # In most of our experiments, the performance on the hold-out data points\n",
        "    # was as good as on the training set. Overfitting was little possible due\n",
        "    # to the small size of the patches.\n",
        "    train_set, val_set = torch.utils.data.random_split(dataset, [4500, 500])\n",
        "    train_loader = data.DataLoader(train_set, batch_size=32, shuffle=True, drop_last=True, num_workers=8)\n",
        "    val_loader = data.DataLoader(val_set, batch_size=32, shuffle=False, drop_last=False, num_workers=4)\n",
        "    \n",
        "    # Create parameter and optimizer\n",
        "    if not isinstance(patch_size, tuple):\n",
        "        patch_size = (patch_size, patch_size)\n",
        "    patch = nn.Parameter(torch.zeros(3, patch_size[0], patch_size[1], device=device), requires_grad=True)\n",
        "    mask = create_circular_mask(patch_size[0], patch_size[1], device)\n",
        "\n",
        "    optimizer = torch.optim.SGD([patch], lr=1e-1, momentum=0.8)\n",
        "    loss_module = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        t = tqdm(train_loader, leave=False)\n",
        "        for img, _ in t:\n",
        "            img = img.to(device)\n",
        "            #apply circular mask to the patch\n",
        "            masked_patch = patch * mask\n",
        "\n",
        "            #place the masked patch on the image\n",
        "            patched_img = place_patch(img.clone(), masked_patch)\n",
        "\n",
        "            pred = model(patched_img)\n",
        "            labels = torch.zeros(img.shape[0], device=pred.device, dtype=torch.long).fill_(target_class)\n",
        "            loss = loss_module(pred, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f\"Epoch {epoch}, Loss: {loss.item():4.2f}\")\n",
        "    \n",
        "    # Final validation\n",
        "    acc, top5 = eval_patch(model, patch, val_loader, target_class)\n",
        "    \n",
        "    return patch.data, {\"acc\": acc.item(), \"top5\": top5.item()}, mask.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get some experience with what to expect from an adversarial patch attack, we want to train multiple patches for different classes. As the training of a patch can take one or two minutes on a GPU, we have provided a couple of pre-trained patches including their results on the full dataset. The results are saved in a JSON file, which is loaded below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load evaluation results of the pretrained patches\n",
        "json_results_file = os.path.join(CHECKPOINT_PATH, \"patch_results.json\")\n",
        "json_results = {}\n",
        "if os.path.isfile(json_results_file):\n",
        "    with open(json_results_file, \"r\") as f:\n",
        "        json_results = json.load(f)\n",
        "        \n",
        "# If you train new patches, you can save the results via calling this function\n",
        "def save_results(patch_dict):\n",
        "    result_dict = {cname: {psize: [t.item() if isinstance(t, torch.Tensor) else t \n",
        "                                   for t in patch_dict[cname][psize][\"results\"]] \n",
        "                           for psize in patch_dict[cname]} \n",
        "                   for cname in patch_dict}\n",
        "    with open(os.path.join(CHECKPOINT_PATH, \"patch_results.json\"), \"w\") as f:\n",
        "        json.dump(result_dict, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a new folder where generated_patches will be saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "PATCHES_PATH = os.path.join(CHECKPOINT_PATH, \"generated_patches\")\n",
        "os.makedirs(PATCHES_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function will be used to save a png image of the patch for visualization purposes and to use on the streamlit app available to test the RES34 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that this function saves the patch with the mask still on it, meaning that it will be a png with the outer section being a circle. We will later use a function that deletes the mask and finally saves the result as a circular image (a sticker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_patch_to_img(patch):\n",
        "    patch = (torch.tanh(patch) + 1) / 2 # Parameter to pixel values\n",
        "    img = patch.cpu().permute(1, 2, 0).numpy()\n",
        "    img = np.clip(img, a_min=0.0, a_max=1.0)\n",
        "    return img\n",
        "\n",
        "def save_patch_png(name, patch_size, patch_dict):\n",
        "    img = convert_patch_to_img(patch_dict[name][patch_size]['patch'])\n",
        "    file_name_as_png = os.path.join(PATCHES_PATH, f\"{name}_{patch_size}_patch.png\")\n",
        "    # Convert the img numpy array to the range [0, 255] and to uint8 type\n",
        "    img_uint8 = (img * 255).astype(np.uint8)\n",
        "    # Create a PIL image from the numpy array\n",
        "    img_pil = Image.fromarray(img_uint8)\n",
        "    # Save the image as a PNG file\n",
        "    img_pil.save(file_name_as_png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function trains call initializes the training of new patches or retrieves data from patches previously trained, depending on whether or not the patch name exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_patches(class_names, patch_sizes):\n",
        "    result_dict = dict()\n",
        "    \n",
        "    # Loop over all classes and patch sizes\n",
        "    for name in class_names:\n",
        "        result_dict[name] = dict()\n",
        "        for patch_size in patch_sizes:\n",
        "            c = label_names.index(name)\n",
        "            file_name = os.path.join(PATCHES_PATH, f\"{name}_{patch_size}_patch.pt\")\n",
        "            # Load patch if pretrained file exists, otherwise start training\n",
        "            if not os.path.isfile(file_name):\n",
        "                #this value mask will be used only after training to generate a png picture of the sticker (rounded patch)\n",
        "                patch, val_results, mask = patch_attack(pretrained_model, target_class=c, patch_size=patch_size, num_epochs=15)\n",
        "                print(f\"Validation results for {name} and {patch_size}:\", val_results)\n",
        "                torch.save(patch, file_name)\n",
        "            else:\n",
        "                #since loading will happen on colab, which only support cpu device, and I created the patches using mpu (much faster)\n",
        "                patch = torch.load(file_name, map_location=torch.device('cpu'))\n",
        "            # Load evaluation results if exist, otherwise manually evaluate the patch\n",
        "            if name in json_results:\n",
        "                results = json_results[name][str(patch_size)]\n",
        "            else:\n",
        "                results = eval_patch(pretrained_model, patch, data_loader, target_class=c)    \n",
        "            \n",
        "            # Store results and the patches in a dict for better access\n",
        "            # Uncomment mask if you are training new patches\n",
        "            result_dict[name][patch_size] = {\n",
        "                \"results\": results,\n",
        "                \"patch\": patch,\n",
        "                #\"mask\": mask\n",
        "            }\n",
        "        \n",
        "    return result_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_names = ['paper towel', 'bulletproof vest']\n",
        "patch_sizes = [32, 48, 64, 128]\n",
        "\n",
        "patch_dict = get_patches(class_names, patch_sizes)\n",
        "#save_results(patch_dict) #Uncomment this if you are training new patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function will save the patch as a circular image, where the mask is made transparent. \n",
        "The cell below also calls a function that saves all the generated patches in the dictionary created above to a png (rectangle version of the patch and a circular one, which corresponds to a sticker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "#Run this after training only not if you are loading current patches and their results\n",
        "def save_circular_patch(name, patch_size, patch_dict):\n",
        "    file_name_as_png = os.path.join(PATCHES_PATH, f\"{name}_{patch_size}_patch_sticker.png\")\n",
        "\n",
        "    # Ensure patch and mask are on CPU and in the correct format\n",
        "    patch = patch_dict[name][patch_size]['patch'].cpu().detach().numpy()\n",
        "    mask = patch_dict[name][patch_size]['mask'].cpu().detach().numpy()\n",
        "\n",
        "    # Transpose to (H, W, C) format for image processing\n",
        "    patch = np.transpose(patch, (1, 2, 0))\n",
        "\n",
        "    # Normalize patch values to [0, 1] range\n",
        "    patch = (patch - patch.min()) / (patch.max() - patch.min())\n",
        "\n",
        "    # Create an RGBA image (Red, Green, Blue, Alpha)\n",
        "    rgba = np.zeros((patch.shape[0], patch.shape[1], 4))\n",
        "    rgba[:,:,:3] = patch\n",
        "    rgba[:,:,3] = mask  # Use the mask for the alpha channel\n",
        "\n",
        "    # Create a PIL Image from the RGBA array\n",
        "    img = Image.fromarray((rgba * 255).astype(np.uint8))\n",
        "    \n",
        "    # Save the image\n",
        "    img.save(file_name_as_png, 'PNG')\n",
        "\n",
        "def save_patches_in_different_formats(class_names, patch_sizes, patch_dict):\n",
        "    for name in class_names:\n",
        "        for patch_size in patch_sizes:\n",
        "            save_patch_png(name, patch_size, patch_dict)\n",
        "            save_circular_patch(name, patch_size, patch_dict)\n",
        "\n",
        "#save_patches_in_different_formats(class_names, patch_sizes, patch_dict) #uncomment after training new patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_patches():\n",
        "    fig, ax = plt.subplots(len(patch_sizes), len(class_names), figsize=(len(class_names)*2.2, len(patch_sizes)*2.2))\n",
        "    for c_idx, cname in enumerate(class_names):\n",
        "        for p_idx, psize in enumerate(patch_sizes):\n",
        "            patch = patch_dict[cname][psize][\"patch\"]\n",
        "            patch = (torch.tanh(patch) + 1) / 2 # Parameter to pixel values\n",
        "            patch = patch.cpu().permute(1, 2, 0).numpy()\n",
        "            patch = np.clip(patch, a_min=0.0, a_max=1.0)\n",
        "            ax[p_idx][c_idx].imshow(patch)\n",
        "            ax[p_idx][c_idx].set_title(f\"{cname}, size {psize}\")\n",
        "            ax[p_idx][c_idx].axis('off')\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "    plt.show()\n",
        "show_patches()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<!-- Some HTML code to increase font size in the following table -->\n",
              "<style>\n",
              "th {font-size: 120%;}\n",
              "td {font-size: 120%;}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<!-- Some HTML code to increase font size in the following table -->\n",
        "<style>\n",
        "th {font-size: 120%;}\n",
        "td {font-size: 120%;}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function that outputs a table in order to display the accuracy of the different patches and their different sizes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tabulate\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_table(top_1=True):\n",
        "    i = 0 if top_1 else 1\n",
        "    table = [[name] + [f\"{(100.0 * patch_dict[name][psize]['results'][i]):4.2f}%\" for psize in patch_sizes]\n",
        "             for name in class_names]\n",
        "    display(HTML(tabulate.tabulate(table, tablefmt='html', headers=[\"Class name\"] + [f\"Patch size {psize}x{psize}\" for psize in patch_sizes])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_table(top_1=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_table(top_1=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function performs the patch attack by placing the patch on different images and seeing how the patch performs, by looking at the top 5 labels the model predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_patch_attack(patch):\n",
        "    patch_batch = exmp_batch.clone()\n",
        "    patch_batch = place_patch(patch_batch, patch)\n",
        "    with torch.no_grad():\n",
        "        patch_preds = pretrained_model(patch_batch.to(device))\n",
        "    for i in range(1,17,5):\n",
        "        show_prediction(patch_batch[i], label_batch[i], patch_preds[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "perform_patch_attack(patch_dict['paper towel'][64]['patch'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "perform_patch_attack(patch_dict['bulletproof vest'][64]['patch'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, I created two adversarial patches of different sizes in order to fool the RES34 model. If the input img contains the patch, then the model would label it as either 'paper towel' or 'bulletproof vest', depending on the class that was selected. \n",
        "\n",
        "During training epoch was set to 15, meaning for every size, the patch will go through 15 iterations of training. The result increased the accuracy (even though for smaller values of epoch, the accuracy was still quite high). Changing the optimizer to Adam did not affect the accuracy a lot. From what I have seen, using the SGD was better, although I haven't tweaked the parameters of each optimizer that much, given the time it takes to generate patches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMtVDE+z81yF7oDhaXsECyb",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
